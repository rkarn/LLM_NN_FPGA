{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83d72e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 16:33:45.240352: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-13 16:33:45.286730: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-13 16:33:45.287364: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-13 16:33:46.060046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "\n",
    "#os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69961520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b721174d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training csv file.\n",
      "Reading Testing csv file.\n",
      "Trainset size:  (175341, 43) (175341,) Testset size:  (82332, 43) (82332,)\n",
      "Shape before one-hot encoding:  (175341,)\n",
      "Shape after one-hot encoding:  (175341, 10)\n"
     ]
    }
   ],
   "source": [
    "# Read in the training CSV file\n",
    "print( \"Reading Training csv file.\")\n",
    "df1 = pd.read_csv(\"UNSW_NB15_training-set.csv\")\n",
    "df1.drop('label', axis=1, inplace=True)\n",
    "\n",
    "#One hot encoding the string variables\n",
    "obj_df1=df1\n",
    "obj_df1[\"proto\"] = obj_df1[\"proto\"].astype('category')\n",
    "obj_df1[\"service\"] = obj_df1[\"service\"].astype('category')\n",
    "obj_df1[\"state\"] = obj_df1[\"state\"].astype('category')\n",
    "obj_df1[\"proto_cat\"] = obj_df1[\"proto\"].cat.codes\n",
    "obj_df1[\"service_cat\"] = obj_df1[\"service\"].cat.codes\n",
    "obj_df1[\"state_cat\"] = obj_df1[\"state\"].cat.codes\n",
    "\n",
    "obj_df1[\"proto\"] = obj_df1[\"proto_cat\"]\n",
    "obj_df1[\"service\"] = obj_df1[\"service_cat\"]\n",
    "obj_df1[\"state\"] = obj_df1[\"state_cat\"]\n",
    "\n",
    "obj_df1.drop('proto_cat', axis=1, inplace=True)\n",
    "obj_df1.drop('service_cat', axis=1, inplace=True)\n",
    "obj_df1.drop('state_cat', axis=1, inplace=True)\n",
    "\n",
    "Y_train_all_attacks = obj_df1[\"attack_cat\"]\n",
    "X_train = obj_df1.values[:,:-1]\n",
    "\n",
    "#Normalizing train set\n",
    "for j in range(0,43):\n",
    "    maximum = max(X_train[:,j])\n",
    "    for i in range(0,len(X_train)):\n",
    "        X_train[i,j] = round(X_train[i,j]/maximum,3)\n",
    "\n",
    "#Make a catagorical cloumn for each type of label in trainset\n",
    "obj_df1=pd.get_dummies(obj_df1, columns=[\"attack_cat\"])\n",
    "Y_train_each_attach = obj_df1.values[:,-10:]\n",
    "        \n",
    "# Read in the testing CSV file \n",
    "print(\"Reading Testing csv file.\")\n",
    "df2 = pd.read_csv(\"UNSW_NB15_testing-set.csv\")\n",
    "df2.drop('label', axis=1, inplace=True)\n",
    "\n",
    "#One hot encoding the string variables\n",
    "obj_df2=df2\n",
    "obj_df2[\"proto\"] = obj_df2[\"proto\"].astype('category')\n",
    "obj_df2[\"service\"] = obj_df2[\"service\"].astype('category')\n",
    "obj_df2[\"state\"] = obj_df2[\"state\"].astype('category')\n",
    "obj_df2[\"proto_cat\"] = obj_df2[\"proto\"].cat.codes\n",
    "obj_df2[\"service_cat\"] = obj_df2[\"service\"].cat.codes\n",
    "obj_df2[\"state_cat\"] = obj_df2[\"state\"].cat.codes\n",
    "\n",
    "obj_df2[\"proto\"] = obj_df2[\"proto_cat\"]\n",
    "obj_df2[\"service\"] = obj_df2[\"service_cat\"]\n",
    "obj_df2[\"state\"] = obj_df2[\"state_cat\"]\n",
    "\n",
    "obj_df2.drop('proto_cat', axis=1, inplace=True)\n",
    "obj_df2.drop('service_cat', axis=1, inplace=True)\n",
    "obj_df2.drop('state_cat', axis=1, inplace=True)\n",
    "\n",
    "Y_test_all_attacks = obj_df2[\"attack_cat\"]\n",
    "X_test = obj_df2.values[:,:-1]\n",
    "\n",
    "#Normalizing test set\n",
    "for j in range(0,43):\n",
    "    maximum = max(X_train[:,j])\n",
    "    for i in range(0,len(X_test)):\n",
    "        X_test[i,j] = round(X_test[i,j]/maximum,3)\n",
    "        \n",
    "#Make a catagorical cloumn for each type of label in testset\n",
    "obj_df2=pd.get_dummies(obj_df2, columns=[\"attack_cat\"])\n",
    "Y_test_each_attach = obj_df2.values[:,-10:]\n",
    "\n",
    "cleanup_nums = {\"Worms\":0, \"Shellcode\":1, \"Reconnaissance\":2, \"Normal\":3, \"Generic\":4, \"Fuzzers\":5, \"Exploits\":6, \"DoS\":7, \"Backdoor\":8, \"Analysis\":9}\n",
    "Y_train_all_attacks.replace(cleanup_nums,inplace=True)\n",
    "Y_test_all_attacks.replace(cleanup_nums,inplace=True)\n",
    "print('Trainset size: ',X_train.shape,Y_train_all_attacks.shape,'Testset size: ',X_test.shape,Y_test_all_attacks.shape)\n",
    "Y_train = Y_train_all_attacks \n",
    "Y_test = Y_test_all_attacks\n",
    "\n",
    "# one-hot encoding using keras' numpy-related utilities\n",
    "n_classes = 10\n",
    "print(\"Shape before one-hot encoding: \", Y_train.shape)\n",
    "Y_train = np_utils.to_categorical(Y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(Y_test, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe16683f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   1/1754 [..............................] - ETA: 8:53 - loss: 2.3132 - accuracy: 0.0500WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0022s). Check your callbacks.\n",
      "1754/1754 [==============================] - 6s 3ms/step - loss: 0.7214 - accuracy: 0.7425 - val_loss: 303597920.0000 - val_accuracy: 0.3039\n",
      "Epoch 2/10\n",
      "1754/1754 [==============================] - 6s 3ms/step - loss: 0.6276 - accuracy: 0.7731 - val_loss: 344225888.0000 - val_accuracy: 0.3293\n",
      "Epoch 3/10\n",
      "1754/1754 [==============================] - 6s 3ms/step - loss: 0.6150 - accuracy: 0.7768 - val_loss: 373325824.0000 - val_accuracy: 0.3504\n",
      "Epoch 4/10\n",
      "1754/1754 [==============================] - 6s 3ms/step - loss: 0.6052 - accuracy: 0.7807 - val_loss: 413484384.0000 - val_accuracy: 0.1237\n",
      "Epoch 5/10\n",
      "1754/1754 [==============================] - 6s 3ms/step - loss: 0.5964 - accuracy: 0.7827 - val_loss: 992014208.0000 - val_accuracy: 0.1571\n",
      "Epoch 6/10\n",
      "1754/1754 [==============================] - 6s 3ms/step - loss: 0.5884 - accuracy: 0.7833 - val_loss: 4083260416.0000 - val_accuracy: 0.0821\n",
      "Epoch 7/10\n",
      "1754/1754 [==============================] - 6s 3ms/step - loss: 0.5820 - accuracy: 0.7853 - val_loss: 5776774656.0000 - val_accuracy: 0.0825\n",
      "Epoch 8/10\n",
      "1754/1754 [==============================] - 6s 3ms/step - loss: 0.5784 - accuracy: 0.7883 - val_loss: 5673527808.0000 - val_accuracy: 0.2775\n",
      "Epoch 9/10\n",
      "1754/1754 [==============================] - 6s 3ms/step - loss: 0.5755 - accuracy: 0.7890 - val_loss: 5884583936.0000 - val_accuracy: 0.2779\n",
      "Epoch 10/10\n",
      "1754/1754 [==============================] - 6s 3ms/step - loss: 0.5730 - accuracy: 0.7903 - val_loss: 6060305920.0000 - val_accuracy: 0.2795\n",
      "2573/2573 - 2s - loss: 6060305920.0000 - accuracy: 0.2795 - 2s/epoch - 769us/step\n",
      "[6060305920.0] [0.27953892946243286]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(43,), activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
    "model_losses = []\n",
    "model_accs = []\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.losses = []\n",
    "            self.batches = []\n",
    "            self.weight_save = []\n",
    "            self.model_weights = model.get_weights()\n",
    "            self.weight_masks = []\n",
    "            for i in range(len(self.model_weights)):\n",
    "                if i%2 == 0:\n",
    "                    self.random_mask = np.logical_and(np.random.randint(0,2,self.model_weights[i].shape), np.random.randint(0,2,self.model_weights[i].shape))\n",
    "                    for j in range(0):\n",
    "                        self.random_mask = np.logical_and(self.random_mask, np.random.randint(0,2,self.model_weights[i].shape))\n",
    "                    self.weight_masks.append(self.random_mask)\n",
    "            for i in range(len(self.model_weights)):\n",
    "                if i == 0:\n",
    "                    self.model_weights[i] = np.multiply(self.model_weights[i], self.weight_masks[int(i/2)])\n",
    "            model.set_weights(self.model_weights)\n",
    "            self.weight_save.append(model.get_weights)       \n",
    "\n",
    "        def on_batch_end(self, batch, logs={}):\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.batches.append(batch)\n",
    "            self.model_weights = model.get_weights()\n",
    "            for i in range(len(self.model_weights)):\n",
    "                if i %2 == 0:\n",
    "                    self.model_weights[i] = np.multiply(self.model_weights[i], self.weight_masks[int(i/2)])\n",
    "            model.set_weights(self.model_weights)\n",
    "            self.weight_save.append(model.get_weights) \n",
    "            \n",
    "l_history=LossHistory()\n",
    "history = model.fit(X_train, Y_train,\n",
    "          batch_size=100, epochs=10, verbose=1,\n",
    "          validation_data=(X_test, Y_test), callbacks = [l_history])\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test, verbose=2)\n",
    "model_losses.append(loss_and_metrics[0])\n",
    "model_accs.append(loss_and_metrics[1])\n",
    "print(model_losses, model_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "385360e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2573/2573 [==============================] - 2s 785us/step\n",
      "Accuracy: 0.27953893990186074\n"
     ]
    }
   ],
   "source": [
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_keras = model.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(accuracy_score(np.argmax(Y_test, axis=1), np.argmax(y_keras, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba223b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_original = np.argmax(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98c29c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset has been saved to 'test_dataset.c'\n"
     ]
    }
   ],
   "source": [
    "# Create the C file\n",
    "with open('UNSW_testdata/test_dataset.c', 'w') as f:\n",
    "    # Write the data samples array\n",
    "    f.write(\"float test_data[%d][%d] = {\\n\" % (X_test.shape[0], X_test.shape[1]))\n",
    "    for row in X_test:\n",
    "        f.write(\"    {%s},\\n\" % \", \".join(f\"{x:.6f}f\" for x in row))\n",
    "    f.write(\"};\\n\\n\")\n",
    "\n",
    "    # Write the labels array\n",
    "    f.write(\"int test_labels[%d] = {\\n\" % len(Y_test_original))\n",
    "    f.write(\"    %s\\n\" % \", \".join(map(str, Y_test_original)))\n",
    "    f.write(\"};\\n\")\n",
    "\n",
    "print(\"Test dataset has been saved to 'test_dataset.c'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a53a6646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights and biases have been saved to 'weights.cpp'\n"
     ]
    }
   ],
   "source": [
    "# Get the weights and biases\n",
    "layer1_weights = model.layers[0].get_weights()[0].T\n",
    "layer1_bias = model.layers[0].get_weights()[1]\n",
    "layer2_weights = model.layers[1].get_weights()[0].T\n",
    "layer2_bias = model.layers[1].get_weights()[1]\n",
    "\n",
    "# Write weights and biases to weights.cpp\n",
    "with open('UNSW_testdata/weights.cpp', 'w') as f:\n",
    "    f.write('#include <vector>\\n\\n')\n",
    "    \n",
    "    f.write('std::vector<std::vector<float>> layer1_weights = {\\n')\n",
    "    for row in layer1_weights:\n",
    "        f.write('    {' + ', '.join(f'{x:.6f}f' for x in row) + '},\\n')\n",
    "    f.write('};\\n\\n')\n",
    "\n",
    "    f.write('std::vector<float> layer1_bias = {')\n",
    "    f.write(', '.join(f'{x:.6f}f' for x in layer1_bias))\n",
    "    f.write('};\\n\\n')\n",
    "\n",
    "    f.write('std::vector<std::vector<float>> layer2_weights = {\\n')\n",
    "    for row in layer2_weights:\n",
    "        f.write('    {' + ', '.join(f'{x:.6f}f' for x in row) + '},\\n')\n",
    "    f.write('};\\n\\n')\n",
    "\n",
    "    f.write('std::vector<float> layer2_bias = {')\n",
    "    f.write(', '.join(f'{x:.6f}f' for x in layer2_bias))\n",
    "    f.write('};\\n')\n",
    "\n",
    "print(\"Weights and biases have been saved to 'weights.cpp'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d474491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -std=c++11 UNSW_testdata/test.cpp -o UNSW_testdata/test_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4046589e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 27.9539%\r\n"
     ]
    }
   ],
   "source": [
    "!./UNSW_testdata/test_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3670a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
